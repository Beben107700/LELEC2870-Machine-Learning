{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sklearn.feature_selection as fsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.read_csv('X1.csv')\n",
    "Y1 = pd.read_csv('Y1.csv', header = None, names = ['shares'])\n",
    "df = X1\n",
    "normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "X1 = normalized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 26.5191068649292 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Finding the mutual information\n",
    "import time\n",
    "start_time = time.time()\n",
    "mutualinfo = fsl.mutual_info_regression(X1, Y1.values.ravel())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011922771533410703\n",
      "[[ 1  7  8 10 20 22 23 24 25 26 27 28 34 36 37 38 39 40 41 46]]\n",
      "(20, 1)\n",
      "[['n_tokens_content']\n",
      " ['num_imgs']\n",
      " ['num_videos']\n",
      " ['num_keywords']\n",
      " ['kw_min_max']\n",
      " ['kw_avg_max']\n",
      " ['kw_min_avg']\n",
      " ['kw_max_avg']\n",
      " ['kw_avg_avg']\n",
      " ['self_reference_min_shares']\n",
      " ['self_reference_max_shares']\n",
      " ['self_reference_avg_sharess']\n",
      " ['weekday_is_saturday']\n",
      " ['is_weekend']\n",
      " ['LDA_00']\n",
      " ['LDA_01']\n",
      " ['LDA_02']\n",
      " ['LDA_03']\n",
      " ['LDA_04']\n",
      " ['rate_positive_words']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-1e1671389faf>:7: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  print(X1.columns[selected_mutinfo].shape)\n",
      "<ipython-input-19-1e1671389faf>:8: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  print(X1.columns[selected_mutinfo])\n"
     ]
    }
   ],
   "source": [
    "sorted_mutualinfo = np.sort(mutualinfo)[::-1]\n",
    "features_to_take = 20\n",
    "threshold = sorted_mutualinfo[features_to_take] \n",
    "print(threshold)\n",
    "selected_mutinfo = np.argwhere(mutualinfo > threshold)\n",
    "print(selected_mutinfo.T)\n",
    "print(X1.columns[selected_mutinfo].shape)\n",
    "print(X1.columns[selected_mutinfo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-4ff6c1840cb4>:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  colnames = (X1.columns[selected_mutinfo]).reshape(-1)\n"
     ]
    }
   ],
   "source": [
    "colnames = (X1.columns[selected_mutinfo]).reshape(-1)\n",
    "#print(colnames)\n",
    "X1_selected = X1\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1_selected, Y1, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1050'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining utility class\n",
    "#by defining this, you only have to write \"for loop\" to load minibatch data\n",
    "class DataLoader(object):\n",
    "    def __init__(self, x, y, batch_size=128, shuffle=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.start_idx = 0\n",
    "        self.data_size = x.shape[0]\n",
    "        if self.shuffle:\n",
    "            self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x, self.y = shuffle(self.x, self.y)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.start_idx >= self.data_size:\n",
    "            if self.shuffle:\n",
    "                self.reset()\n",
    "            self.start_idx = 0\n",
    "            raise StopIteration\n",
    "    \n",
    "        batch_x = self.x[self.start_idx:self.start_idx+self.batch_size]\n",
    "        batch_y = self.y[self.start_idx:self.start_idx+self.batch_size]\n",
    "\n",
    "        batch_x = torch.tensor(batch_x, dtype=torch.float, device=device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float, device=device)\n",
    "\n",
    "        self.start_idx += self.batch_size\n",
    "\n",
    "        return (batch_x,batch_y)\n",
    "\n",
    "#defining MLP model\n",
    "#generally out_dim is more than 1, but this model only allows 1.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim=1):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        assert out_dim==1, 'out_dim must be 1'\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.linear1 = nn.Linear(self.in_dim, self.hidden_dim)\n",
    "        self.linear2 = nn.Linear(self.hidden_dim, self.out_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        hidden = self.linear1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.linear2(relu)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to np array\n",
    "x_train = X_train.values\n",
    "y_train = y_train.values\n",
    "x_valid = X_test.values\n",
    "y_valid = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate model\n",
    "mlp = MLP(x_train.shape[1], 64, 1).to(device)\n",
    "optimizer = optim.Adam(mlp.parameters())\n",
    "train_dataloader = DataLoader(x_train, y_train, batch_size=64)\n",
    "valid_dataloader = DataLoader(x_valid, y_valid, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model learns to minimize MAE\n",
    "def mae_loss(y_pred, y_true):\n",
    "    mae = torch.abs(y_true - y_pred).mean()\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:1 Batch:0/248 Loss:3733.7080 Time:0m0.02s\r",
      "Epoch:1 Batch:1/248 Loss:2466.5515 Time:0m0.03s\r",
      "Epoch:1 Batch:2/248 Loss:4107.2217 Time:0m0.03s\r",
      "Epoch:1 Batch:3/248 Loss:2596.5967 Time:0m0.03s\r",
      "Epoch:1 Batch:4/248 Loss:3244.3413 Time:0m0.03s\r",
      "Epoch:1 Batch:5/248 Loss:2393.1492 Time:0m0.03s\r",
      "Epoch:1 Batch:6/248 Loss:2143.9980 Time:0m0.03s\r",
      "Epoch:1 Batch:7/248 Loss:2236.1421 Time:0m0.04s\r",
      "Epoch:1 Batch:8/248 Loss:3237.8694 Time:0m0.04s\r",
      "Epoch:1 Batch:9/248 Loss:3595.3354 Time:0m0.04s\r",
      "Epoch:1 Batch:10/248 Loss:4173.6646 Time:0m0.04s\r",
      "Epoch:1 Batch:11/248 Loss:2476.6460 Time:0m0.04s\r",
      "Epoch:1 Batch:12/248 Loss:2096.8984 Time:0m0.04s\r",
      "Epoch:1 Batch:13/248 Loss:3950.5996 Time:0m0.04s\r",
      "Epoch:1 Batch:14/248 Loss:2942.6602 Time:0m0.04s\r",
      "Epoch:1 Batch:15/248 Loss:5576.4541 Time:0m0.05s\r",
      "Epoch:1 Batch:16/248 Loss:2700.6790 Time:0m0.05s\r",
      "Epoch:1 Batch:17/248 Loss:2954.3113 Time:0m0.05s\r",
      "Epoch:1 Batch:18/248 Loss:5344.3726 Time:0m0.05s\r",
      "Epoch:1 Batch:19/248 Loss:5135.4502 Time:0m0.05s\r",
      "Epoch:1 Batch:20/248 Loss:3100.1318 Time:0m0.05s\r",
      "Epoch:1 Batch:21/248 Loss:3249.2163 Time:0m0.05s\r",
      "Epoch:1 Batch:22/248 Loss:3108.3694 Time:0m0.05s\r",
      "Epoch:1 Batch:23/248 Loss:2738.5093 Time:0m0.06s\r",
      "Epoch:1 Batch:24/248 Loss:3872.9639 Time:0m0.06s\r",
      "Epoch:1 Batch:25/248 Loss:2654.1650 Time:0m0.06s\r",
      "Epoch:1 Batch:26/248 Loss:9726.2295 Time:0m0.06s\r",
      "Epoch:1 Batch:27/248 Loss:2885.4531 Time:0m0.06s\r",
      "Epoch:1 Batch:28/248 Loss:2155.6838 Time:0m0.06s\r",
      "Epoch:1 Batch:29/248 Loss:4780.3467 Time:0m0.06s\r",
      "Epoch:1 Batch:30/248 Loss:2284.8091 Time:0m0.06s\r",
      "Epoch:1 Batch:31/248 Loss:2933.6968 Time:0m0.06s\r",
      "Epoch:1 Batch:32/248 Loss:3391.8350 Time:0m0.06s\r",
      "Epoch:1 Batch:33/248 Loss:3315.6440 Time:0m0.07s\r",
      "Epoch:1 Batch:34/248 Loss:4119.3931 Time:0m0.07s\r",
      "Epoch:1 Batch:35/248 Loss:5845.4688 Time:0m0.07s\r",
      "Epoch:1 Batch:36/248 Loss:3054.7742 Time:0m0.07s\r",
      "Epoch:1 Batch:37/248 Loss:2778.1602 Time:0m0.07s\r",
      "Epoch:1 Batch:38/248 Loss:3474.2791 Time:0m0.07s\r",
      "Epoch:1 Batch:39/248 Loss:2353.4597 Time:0m0.07s\r",
      "Epoch:1 Batch:40/248 Loss:2121.9280 Time:0m0.07s\r",
      "Epoch:1 Batch:41/248 Loss:3889.2288 Time:0m0.07s\r",
      "Epoch:1 Batch:42/248 Loss:2541.8286 Time:0m0.07s\r",
      "Epoch:1 Batch:43/248 Loss:3530.9607 Time:0m0.08s\r",
      "Epoch:1 Batch:44/248 Loss:7388.9526 Time:0m0.08s\r",
      "Epoch:1 Batch:45/248 Loss:2566.3184 Time:0m0.08s\r",
      "Epoch:1 Batch:46/248 Loss:7267.8574 Time:0m0.08s\r",
      "Epoch:1 Batch:47/248 Loss:5264.2305 Time:0m0.08s\r",
      "Epoch:1 Batch:48/248 Loss:2763.4036 Time:0m0.08s\r",
      "Epoch:1 Batch:49/248 Loss:2904.7378 Time:0m0.08s\r",
      "Epoch:1 Batch:50/248 Loss:2091.8301 Time:0m0.08s\r",
      "Epoch:1 Batch:51/248 Loss:2485.2625 Time:0m0.09s\r",
      "Epoch:1 Batch:52/248 Loss:3812.9739 Time:0m0.09s\r",
      "Epoch:1 Batch:53/248 Loss:5200.8027 Time:0m0.09s\r",
      "Epoch:1 Batch:54/248 Loss:3284.0698 Time:0m0.09s\r",
      "Epoch:1 Batch:55/248 Loss:2545.7556 Time:0m0.09s\r",
      "Epoch:1 Batch:56/248 Loss:2593.7886 Time:0m0.09s\r",
      "Epoch:1 Batch:57/248 Loss:2729.8145 Time:0m0.09s\r",
      "Epoch:1 Batch:58/248 Loss:3936.2227 Time:0m0.09s\r",
      "Epoch:1 Batch:59/248 Loss:3671.6548 Time:0m0.09s\r",
      "Epoch:1 Batch:60/248 Loss:3902.1553 Time:0m0.10s\r",
      "Epoch:1 Batch:61/248 Loss:4016.5916 Time:0m0.10s\r",
      "Epoch:1 Batch:62/248 Loss:3531.3457 Time:0m0.10s\r",
      "Epoch:1 Batch:63/248 Loss:2998.4985 Time:0m0.10s\r",
      "Epoch:1 Batch:64/248 Loss:4425.2915 Time:0m0.10s\r",
      "Epoch:1 Batch:65/248 Loss:3269.2065 Time:0m0.11s\r",
      "Epoch:1 Batch:66/248 Loss:3255.7144 Time:0m0.11s\r",
      "Epoch:1 Batch:67/248 Loss:2180.9939 Time:0m0.11s\r",
      "Epoch:1 Batch:68/248 Loss:2887.0164 Time:0m0.11s\r",
      "Epoch:1 Batch:69/248 Loss:2645.2759 Time:0m0.11s\r",
      "Epoch:1 Batch:70/248 Loss:1859.5833 Time:0m0.11s\r",
      "Epoch:1 Batch:71/248 Loss:3580.5217 Time:0m0.12s\r",
      "Epoch:1 Batch:72/248 Loss:2799.9299 Time:0m0.12s\r",
      "Epoch:1 Batch:73/248 Loss:3977.1003 Time:0m0.12s\r",
      "Epoch:1 Batch:74/248 Loss:2169.3418 Time:0m0.12s\r",
      "Epoch:1 Batch:75/248 Loss:2587.9001 Time:0m0.12s\r",
      "Epoch:1 Batch:76/248 Loss:2625.1772 Time:0m0.12s\r",
      "Epoch:1 Batch:77/248 Loss:2925.9985 Time:0m0.12s\r",
      "Epoch:1 Batch:78/248 Loss:4766.4023 Time:0m0.12s\r",
      "Epoch:1 Batch:79/248 Loss:1951.9008 Time:0m0.12s\r",
      "Epoch:1 Batch:80/248 Loss:5624.7314 Time:0m0.12s\r",
      "Epoch:1 Batch:81/248 Loss:4223.6440 Time:0m0.12s\r",
      "Epoch:1 Batch:82/248 Loss:3271.5322 Time:0m0.12s\r",
      "Epoch:1 Batch:83/248 Loss:2160.8267 Time:0m0.12s\r",
      "Epoch:1 Batch:84/248 Loss:2385.0913 Time:0m0.13s\r",
      "Epoch:1 Batch:85/248 Loss:2841.5081 Time:0m0.13s\r",
      "Epoch:1 Batch:86/248 Loss:2344.8677 Time:0m0.13s\r",
      "Epoch:1 Batch:87/248 Loss:3210.3013 Time:0m0.13s\r",
      "Epoch:1 Batch:88/248 Loss:4659.3208 Time:0m0.13s\r",
      "Epoch:1 Batch:89/248 Loss:3763.5906 Time:0m0.13s\r",
      "Epoch:1 Batch:90/248 Loss:2978.4189 Time:0m0.13s\r",
      "Epoch:1 Batch:91/248 Loss:2516.3611 Time:0m0.13s\r",
      "Epoch:1 Batch:92/248 Loss:3174.5139 Time:0m0.13s\r",
      "Epoch:1 Batch:93/248 Loss:2908.5508 Time:0m0.13s\r",
      "Epoch:1 Batch:94/248 Loss:2401.6060 Time:0m0.14s\r",
      "Epoch:1 Batch:95/248 Loss:7326.9185 Time:0m0.14s\r",
      "Epoch:1 Batch:96/248 Loss:4173.1987 Time:0m0.14s\r",
      "Epoch:1 Batch:97/248 Loss:3653.5688 Time:0m0.14s\r",
      "Epoch:1 Batch:98/248 Loss:3704.2925 Time:0m0.14s\r",
      "Epoch:1 Batch:99/248 Loss:3562.1331 Time:0m0.14s\r",
      "Epoch:1 Batch:100/248 Loss:3663.3611 Time:0m0.14s\r",
      "Epoch:1 Batch:101/248 Loss:2878.3740 Time:0m0.14s\r",
      "Epoch:1 Batch:102/248 Loss:2659.6553 Time:0m0.14s\r",
      "Epoch:1 Batch:103/248 Loss:3113.4778 Time:0m0.14s\r",
      "Epoch:1 Batch:104/248 Loss:2694.1294 Time:0m0.14s\r",
      "Epoch:1 Batch:105/248 Loss:3961.5698 Time:0m0.14s\r",
      "Epoch:1 Batch:106/248 Loss:3755.3179 Time:0m0.15s\r",
      "Epoch:1 Batch:107/248 Loss:3197.7148 Time:0m0.15s\r",
      "Epoch:1 Batch:108/248 Loss:3030.0791 Time:0m0.15s\r",
      "Epoch:1 Batch:109/248 Loss:3675.2729 Time:0m0.15s\r",
      "Epoch:1 Batch:110/248 Loss:2541.9377 Time:0m0.15s\r",
      "Epoch:1 Batch:111/248 Loss:4087.9536 Time:0m0.15s\r",
      "Epoch:1 Batch:112/248 Loss:2205.0188 Time:0m0.15s\r",
      "Epoch:1 Batch:113/248 Loss:5422.1948 Time:0m0.15s\r",
      "Epoch:1 Batch:114/248 Loss:3203.2891 Time:0m0.15s\r",
      "Epoch:1 Batch:115/248 Loss:2806.9597 Time:0m0.15s\r",
      "Epoch:1 Batch:116/248 Loss:2610.1079 Time:0m0.15s\r",
      "Epoch:1 Batch:117/248 Loss:3653.7961 Time:0m0.15s\r",
      "Epoch:1 Batch:118/248 Loss:4770.3477 Time:0m0.16s\r",
      "Epoch:1 Batch:119/248 Loss:2630.3994 Time:0m0.16s\r",
      "Epoch:1 Batch:120/248 Loss:3187.2571 Time:0m0.16s\r",
      "Epoch:1 Batch:121/248 Loss:3151.5701 Time:0m0.16s\r",
      "Epoch:1 Batch:122/248 Loss:4487.9492 Time:0m0.16s\r",
      "Epoch:1 Batch:123/248 Loss:4178.0576 Time:0m0.16s\r",
      "Epoch:1 Batch:124/248 Loss:4483.6743 Time:0m0.16s\r",
      "Epoch:1 Batch:125/248 Loss:3874.3298 Time:0m0.16s\r",
      "Epoch:1 Batch:126/248 Loss:3959.0750 Time:0m0.16s\r",
      "Epoch:1 Batch:127/248 Loss:5291.0986 Time:0m0.16s\r",
      "Epoch:1 Batch:128/248 Loss:2678.6333 Time:0m0.16s\r",
      "Epoch:1 Batch:129/248 Loss:3914.5039 Time:0m0.17s\r",
      "Epoch:1 Batch:130/248 Loss:2621.9622 Time:0m0.17s\r",
      "Epoch:1 Batch:131/248 Loss:3492.9983 Time:0m0.17s\r",
      "Epoch:1 Batch:132/248 Loss:3243.3179 Time:0m0.17s\r",
      "Epoch:1 Batch:133/248 Loss:3087.2932 Time:0m0.17s\r",
      "Epoch:1 Batch:134/248 Loss:3210.7957 Time:0m0.17s\r",
      "Epoch:1 Batch:135/248 Loss:3206.6843 Time:0m0.17s\r",
      "Epoch:1 Batch:136/248 Loss:3077.1284 Time:0m0.17s\r",
      "Epoch:1 Batch:137/248 Loss:3352.7932 Time:0m0.17s\r",
      "Epoch:1 Batch:138/248 Loss:1640.0945 Time:0m0.17s\r",
      "Epoch:1 Batch:139/248 Loss:3142.7395 Time:0m0.17s\r",
      "Epoch:1 Batch:140/248 Loss:2185.8411 Time:0m0.18s\r",
      "Epoch:1 Batch:141/248 Loss:6554.7944 Time:0m0.18s\r",
      "Epoch:1 Batch:142/248 Loss:7597.7012 Time:0m0.18s\r",
      "Epoch:1 Batch:143/248 Loss:2334.9570 Time:0m0.18s\r",
      "Epoch:1 Batch:144/248 Loss:3068.0500 Time:0m0.18s\r",
      "Epoch:1 Batch:145/248 Loss:2725.0513 Time:0m0.18s\r",
      "Epoch:1 Batch:146/248 Loss:2632.3154 Time:0m0.18s\r",
      "Epoch:1 Batch:147/248 Loss:3213.3528 Time:0m0.18s\r",
      "Epoch:1 Batch:148/248 Loss:4007.8069 Time:0m0.18s\r",
      "Epoch:1 Batch:149/248 Loss:3517.9365 Time:0m0.18s\r",
      "Epoch:1 Batch:150/248 Loss:2758.6001 Time:0m0.19s\r",
      "Epoch:1 Batch:151/248 Loss:2579.5093 Time:0m0.19s\r",
      "Epoch:1 Batch:152/248 Loss:2146.5242 Time:0m0.19s\r",
      "Epoch:1 Batch:153/248 Loss:3349.0801 Time:0m0.19s\r",
      "Epoch:1 Batch:154/248 Loss:4992.5317 Time:0m0.19s\r",
      "Epoch:1 Batch:155/248 Loss:2737.1497 Time:0m0.19s\r",
      "Epoch:1 Batch:156/248 Loss:2508.5205 Time:0m0.19s\r",
      "Epoch:1 Batch:157/248 Loss:2879.0715 Time:0m0.19s\r",
      "Epoch:1 Batch:158/248 Loss:2403.1196 Time:0m0.19s\r",
      "Epoch:1 Batch:159/248 Loss:3686.5083 Time:0m0.19s\r",
      "Epoch:1 Batch:160/248 Loss:3201.4077 Time:0m0.19s\r",
      "Epoch:1 Batch:161/248 Loss:2837.7312 Time:0m0.20s\r",
      "Epoch:1 Batch:162/248 Loss:4450.3281 Time:0m0.20s\r",
      "Epoch:1 Batch:163/248 Loss:2671.3779 Time:0m0.20s\r",
      "Epoch:1 Batch:164/248 Loss:3087.8838 Time:0m0.20s\r",
      "Epoch:1 Batch:165/248 Loss:2875.7117 Time:0m0.20s\r",
      "Epoch:1 Batch:166/248 Loss:3221.2683 Time:0m0.20s\r",
      "Epoch:1 Batch:167/248 Loss:2644.1284 Time:0m0.20s\r",
      "Epoch:1 Batch:168/248 Loss:2052.4473 Time:0m0.20s\r",
      "Epoch:1 Batch:169/248 Loss:2434.4255 Time:0m0.20s\r",
      "Epoch:1 Batch:170/248 Loss:2699.0649 Time:0m0.20s\r",
      "Epoch:1 Batch:171/248 Loss:3738.4531 Time:0m0.21s\r",
      "Epoch:1 Batch:172/248 Loss:3799.8625 Time:0m0.21s\r",
      "Epoch:1 Batch:173/248 Loss:5612.9043 Time:0m0.21s\r",
      "Epoch:1 Batch:174/248 Loss:2788.2983 Time:0m0.21s\r",
      "Epoch:1 Batch:175/248 Loss:4924.6152 Time:0m0.21s\r",
      "Epoch:1 Batch:176/248 Loss:4827.7568 Time:0m0.21s\r",
      "Epoch:1 Batch:177/248 Loss:3130.4163 Time:0m0.21s\r",
      "Epoch:1 Batch:178/248 Loss:2674.8015 Time:0m0.21s\r",
      "Epoch:1 Batch:179/248 Loss:2246.4883 Time:0m0.21s\r",
      "Epoch:1 Batch:180/248 Loss:2395.0337 Time:0m0.21s\r",
      "Epoch:1 Batch:181/248 Loss:3199.9485 Time:0m0.21s\r",
      "Epoch:1 Batch:182/248 Loss:4312.3491 Time:0m0.22s\r",
      "Epoch:1 Batch:183/248 Loss:2484.6108 Time:0m0.22s\r",
      "Epoch:1 Batch:184/248 Loss:3536.2087 Time:0m0.22s\r",
      "Epoch:1 Batch:185/248 Loss:2814.3955 Time:0m0.22s\r",
      "Epoch:1 Batch:186/248 Loss:2461.6816 Time:0m0.22s\r",
      "Epoch:1 Batch:187/248 Loss:4855.3438 Time:0m0.22s\r",
      "Epoch:1 Batch:188/248 Loss:2455.1440 Time:0m0.22s\r",
      "Epoch:1 Batch:189/248 Loss:3511.3474 Time:0m0.22s\r",
      "Epoch:1 Batch:190/248 Loss:3184.8723 Time:0m0.22s\r",
      "Epoch:1 Batch:191/248 Loss:3317.6604 Time:0m0.22s\r",
      "Epoch:1 Batch:192/248 Loss:2994.1982 Time:0m0.22s\r",
      "Epoch:1 Batch:193/248 Loss:2234.6123 Time:0m0.22s\r",
      "Epoch:1 Batch:194/248 Loss:2680.6394 Time:0m0.23s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Batch:247/248 Loss:3957.4250 Time:0m0.29ss\n",
      "Valid Loss:3397.4319\n",
      "Epoch:2 Batch:247/248 Loss:2930.8364 Time:0m0.28ss\n",
      "Valid Loss:3164.5577\n",
      "Epoch:3 Batch:247/248 Loss:1831.8425 Time:0m0.29s\n",
      "Valid Loss:2808.9880\n",
      "Epoch:4 Batch:247/248 Loss:1661.0052 Time:0m0.28ss\n",
      "Valid Loss:2587.2792\n",
      "Epoch:5 Batch:247/248 Loss:1974.6406 Time:0m0.28s\n",
      "Valid Loss:2526.3523\n",
      "Epoch:6 Batch:247/248 Loss:1764.7166 Time:0m0.28ss\n",
      "Valid Loss:2512.1218\n",
      "Epoch:7 Batch:247/248 Loss:1506.4657 Time:0m0.30ss\n",
      "Valid Loss:2509.8240\n",
      "Epoch:8 Batch:247/248 Loss:1900.6609 Time:0m0.30s\n",
      "Valid Loss:2504.7994\n",
      "Epoch:9 Batch:247/248 Loss:3312.7700 Time:0m0.30ss\n",
      "Valid Loss:2504.2069\n",
      "Epoch:10 Batch:247/248 Loss:1887.2942 Time:0m0.30s\n",
      "Valid Loss:2501.7878\n",
      "Epoch:11 Batch:247/248 Loss:4534.7729 Time:0m0.27s\n",
      "Valid Loss:2497.7053\n",
      "Epoch:12 Batch:247/248 Loss:2613.0032 Time:0m0.28ss\n",
      "Valid Loss:2496.8532\n",
      "Epoch:13 Batch:247/248 Loss:2578.5483 Time:0m0.29s\n",
      "Valid Loss:2492.1532\n",
      "Epoch:14 Batch:247/248 Loss:2621.7754 Time:0m0.29ss\n",
      "Valid Loss:2491.8440\n",
      "Epoch:15 Batch:247/248 Loss:1648.9312 Time:0m0.28ss\n",
      "Valid Loss:2487.2093\n",
      "Epoch:16 Batch:247/248 Loss:4866.9087 Time:0m0.29s\n",
      "Valid Loss:2486.5833\n",
      "Epoch:17 Batch:247/248 Loss:1538.3124 Time:0m0.31ss\n",
      "Valid Loss:2483.0200\n",
      "Epoch:18 Batch:247/248 Loss:4487.9902 Time:0m0.32ss\n",
      "Valid Loss:2480.8164\n",
      "Epoch:19 Batch:247/248 Loss:1892.4105 Time:0m0.29s\n",
      "Valid Loss:2479.5901\n",
      "Epoch:20 Batch:247/248 Loss:2532.2273 Time:0m0.31ss\n",
      "Valid Loss:2479.0611\n",
      "Epoch:21 Batch:247/248 Loss:3755.1458 Time:0m0.32s\n",
      "Valid Loss:2477.2220\n",
      "Epoch:22 Batch:247/248 Loss:1665.0985 Time:0m0.36ss\n",
      "Valid Loss:2475.5656\n",
      "Epoch:23 Batch:247/248 Loss:984.9167 Time:0m0.35sss\n",
      "Valid Loss:2473.4896\n",
      "Epoch:24 Batch:247/248 Loss:2966.1707 Time:0m0.28ss\n",
      "Valid Loss:2473.2543\n",
      "Epoch:25 Batch:247/248 Loss:1463.9100 Time:0m0.28ss\n",
      "Valid Loss:2472.0178\n",
      "Epoch:26 Batch:247/248 Loss:1218.3918 Time:0m0.30ss\n",
      "Valid Loss:2471.0570\n",
      "Epoch:27 Batch:247/248 Loss:1663.7023 Time:0m0.32s\n",
      "Valid Loss:2469.5707\n",
      "Epoch:28 Batch:247/248 Loss:4050.4272 Time:0m0.31ss\n",
      "Valid Loss:2468.6332\n",
      "Epoch:29 Batch:247/248 Loss:3329.6072 Time:0m0.36s\n",
      "Valid Loss:2468.8401\n",
      "Epoch:30 Batch:247/248 Loss:1824.0387 Time:0m0.30s\n",
      "Valid Loss:2467.8209\n",
      "Epoch:31 Batch:247/248 Loss:1285.5400 Time:0m0.31s\n",
      "Valid Loss:2467.1594\n",
      "Epoch:32 Batch:247/248 Loss:2077.3801 Time:0m0.28s\n",
      "Valid Loss:2466.4033\n",
      "Epoch:33 Batch:247/248 Loss:2510.5103 Time:0m0.27ss\n",
      "Valid Loss:2466.1090\n",
      "Epoch:34 Batch:247/248 Loss:2639.8191 Time:0m0.28ss\n",
      "Valid Loss:2465.6902\n",
      "Epoch:35 Batch:247/248 Loss:3133.2229 Time:0m0.29s\n",
      "Valid Loss:2466.5161\n",
      "Epoch:36 Batch:247/248 Loss:3597.6565 Time:0m0.28ss\n",
      "Valid Loss:2465.5862\n",
      "Epoch:37 Batch:247/248 Loss:1586.6218 Time:0m0.29ss\n",
      "Valid Loss:2464.3971\n",
      "Epoch:38 Batch:247/248 Loss:2784.9919 Time:0m0.27ss\n",
      "Valid Loss:2464.3651\n",
      "Epoch:39 Batch:247/248 Loss:1173.3058 Time:0m0.27ss\n",
      "Valid Loss:2463.9632\n",
      "Epoch:40 Batch:247/248 Loss:3224.4146 Time:0m0.27ss\n",
      "Valid Loss:2463.8843\n",
      "Epoch:41 Batch:247/248 Loss:4212.0454 Time:0m0.28s\n",
      "Valid Loss:2464.1604\n",
      "Epoch:42 Batch:247/248 Loss:1595.4153 Time:0m0.29s\n",
      "Valid Loss:2463.8223\n",
      "Epoch:43 Batch:247/248 Loss:2185.8970 Time:0m0.27s\n",
      "Valid Loss:2463.4962\n",
      "Epoch:44 Batch:247/248 Loss:1433.0934 Time:0m0.31s\n",
      "Valid Loss:2462.1551\n",
      "Epoch:45 Batch:247/248 Loss:2500.6423 Time:0m0.30ss\n",
      "Valid Loss:2461.8367\n",
      "Epoch:46 Batch:247/248 Loss:1735.3047 Time:0m0.29ss\n",
      "Valid Loss:2461.6276\n",
      "Epoch:47 Batch:247/248 Loss:2084.1353 Time:0m0.31s\n",
      "Valid Loss:2461.0054\n",
      "Epoch:48 Batch:247/248 Loss:1614.6791 Time:0m0.28ss\n",
      "Valid Loss:2460.8209\n",
      "Epoch:49 Batch:247/248 Loss:1755.6605 Time:0m0.27s\n",
      "Valid Loss:2460.6172\n",
      "Epoch:50 Batch:247/248 Loss:2114.5830 Time:0m0.29ss\n",
      "Valid Loss:2460.6012\n",
      "Epoch:51 Batch:247/248 Loss:1806.4788 Time:0m0.28ss\n",
      "Valid Loss:2460.4744\n",
      "Epoch:52 Batch:247/248 Loss:4160.2773 Time:0m0.30ss\n",
      "Valid Loss:2460.3846\n",
      "Epoch:53 Batch:247/248 Loss:3846.9167 Time:0m0.28s\n",
      "Valid Loss:2459.5073\n",
      "Epoch:54 Batch:247/248 Loss:2679.7007 Time:0m0.28s\n",
      "Valid Loss:2459.5894\n",
      "Epoch:55 Batch:247/248 Loss:2070.7896 Time:0m0.32ss\n",
      "Valid Loss:2460.7055\n",
      "Epoch:56 Batch:247/248 Loss:2522.0168 Time:0m0.29ss\n",
      "Valid Loss:2458.9322\n",
      "Epoch:57 Batch:247/248 Loss:3780.7471 Time:0m0.28s\n",
      "Valid Loss:2459.8083\n",
      "Epoch:58 Batch:247/248 Loss:1132.5194 Time:0m0.28ss\n",
      "Valid Loss:2459.3813\n",
      "Epoch:59 Batch:247/248 Loss:3342.9167 Time:0m0.32ss\n",
      "Valid Loss:2459.0937\n",
      "Epoch:60 Batch:247/248 Loss:1020.9312 Time:0m0.30ss\n",
      "Valid Loss:2458.2544\n",
      "Epoch:61 Batch:247/248 Loss:3938.3865 Time:0m0.31s\n",
      "Valid Loss:2459.4709\n",
      "Epoch:62 Batch:247/248 Loss:2762.9187 Time:0m0.29ss\n",
      "Valid Loss:2469.6426\n",
      "Epoch:63 Batch:247/248 Loss:1739.8976 Time:0m0.28ss\n",
      "Valid Loss:2458.1603\n",
      "Epoch:64 Batch:247/248 Loss:2067.6675 Time:0m0.29ss\n",
      "Valid Loss:2459.1149\n",
      "Epoch:65 Batch:247/248 Loss:1559.5334 Time:0m0.31s\n",
      "Valid Loss:2458.3787\n",
      "Epoch:66 Batch:247/248 Loss:2377.2322 Time:0m0.28s\n",
      "Valid Loss:2458.4030\n",
      "Epoch:67 Batch:247/248 Loss:1685.0520 Time:0m0.29ss\n",
      "Valid Loss:2457.1644\n",
      "Epoch:68 Batch:247/248 Loss:2541.1721 Time:0m0.31ss\n",
      "Valid Loss:2457.3744\n",
      "Epoch:69 Batch:247/248 Loss:1534.8351 Time:0m0.28s\n",
      "Valid Loss:2457.3330\n",
      "Epoch:70 Batch:247/248 Loss:1350.7821 Time:0m0.30s\n",
      "Valid Loss:2456.7672\n",
      "Epoch:71 Batch:247/248 Loss:4119.0703 Time:0m0.29s\n",
      "Valid Loss:2456.8939\n",
      "Epoch:72 Batch:247/248 Loss:2315.3181 Time:0m0.27ss\n",
      "Valid Loss:2457.3475\n",
      "Epoch:73 Batch:247/248 Loss:2204.2463 Time:0m0.28s\n",
      "Valid Loss:2456.8777\n",
      "Epoch:74 Batch:247/248 Loss:1230.8750 Time:0m0.33s\n",
      "Valid Loss:2457.1793\n",
      "Epoch:75 Batch:247/248 Loss:791.8560 Time:0m0.31sss\n",
      "Valid Loss:2456.4358\n",
      "Epoch:76 Batch:247/248 Loss:1640.1130 Time:0m0.28ss\n",
      "Valid Loss:2456.8273\n",
      "Epoch:77 Batch:247/248 Loss:4893.2100 Time:0m0.30ss\n",
      "Valid Loss:2457.3261\n",
      "Epoch:78 Batch:247/248 Loss:4479.0371 Time:0m0.32ss\n",
      "Valid Loss:2455.2929\n",
      "Epoch:79 Batch:247/248 Loss:2809.2566 Time:0m0.29s\n",
      "Valid Loss:2455.2422\n",
      "Epoch:80 Batch:247/248 Loss:1885.8427 Time:0m0.28s\n",
      "Valid Loss:2456.6022\n",
      "Epoch:81 Batch:247/248 Loss:1522.4248 Time:0m0.28s\n",
      "Valid Loss:2455.7170\n",
      "Epoch:82 Batch:247/248 Loss:2228.2004 Time:0m0.28ss\n",
      "Valid Loss:2456.3931\n",
      "Epoch:83 Batch:247/248 Loss:2074.3521 Time:0m0.28ss\n",
      "Valid Loss:2454.7167\n",
      "Epoch:84 Batch:247/248 Loss:1528.1375 Time:0m0.28ss\n",
      "Valid Loss:2455.5457\n",
      "Epoch:85 Batch:247/248 Loss:3076.5820 Time:0m0.28s\n",
      "Valid Loss:2454.8524\n",
      "Epoch:86 Batch:247/248 Loss:1903.6788 Time:0m0.28ss\n",
      "Valid Loss:2455.4646\n",
      "Epoch:87 Batch:247/248 Loss:1410.1003 Time:0m0.30s\n",
      "Valid Loss:2455.0878\n",
      "Epoch:88 Batch:247/248 Loss:1708.2677 Time:0m0.28ss\n",
      "Valid Loss:2455.3067\n",
      "Epoch:89 Batch:247/248 Loss:1464.5587 Time:0m0.28ss\n",
      "Valid Loss:2455.7537\n",
      "Epoch:90 Batch:247/248 Loss:2095.2415 Time:0m0.30ss\n",
      "Valid Loss:2455.0120\n",
      "Epoch:91 Batch:247/248 Loss:4568.3545 Time:0m0.30ss\n",
      "Valid Loss:2453.8242\n",
      "Epoch:92 Batch:247/248 Loss:1635.6636 Time:0m0.28s\n",
      "Valid Loss:2454.0648\n",
      "Epoch:93 Batch:247/248 Loss:2993.4417 Time:0m0.32s\n",
      "Valid Loss:2454.2189\n",
      "Epoch:94 Batch:247/248 Loss:2213.7820 Time:0m0.30ss\n",
      "Valid Loss:2453.8896\n",
      "Epoch:95 Batch:247/248 Loss:1693.4202 Time:0m0.29ss\n",
      "Valid Loss:2453.7772\n",
      "Epoch:96 Batch:247/248 Loss:2347.9851 Time:0m0.28ss\n",
      "Valid Loss:2453.6408\n",
      "Epoch:97 Batch:247/248 Loss:1335.8423 Time:0m0.30s\n",
      "Valid Loss:2454.1525\n",
      "Epoch:98 Batch:247/248 Loss:1436.8531 Time:0m0.28s\n",
      "Valid Loss:2453.8167\n",
      "Epoch:99 Batch:247/248 Loss:2423.2698 Time:0m0.33s\n",
      "Valid Loss:2453.8614\n",
      "Epoch:100 Batch:247/248 Loss:1866.9343 Time:0m0.31ss\n",
      "Valid Loss:2453.5182\n"
     ]
    }
   ],
   "source": [
    "#training phase\n",
    "epochs = 100\n",
    "#to plot loss curve after training\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    mlp.train()\n",
    "    num_batch = train_dataloader.data_size // train_dataloader.batch_size + 1\n",
    "    \n",
    "    for batch_id, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "        \n",
    "        y_pred = mlp(batch_x)\n",
    "\n",
    "        loss = mae_loss(y_pred, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        elapsed_min = int(elapsed_time / 60)\n",
    "        elapsed_sec = elapsed_time - 60 * elapsed_min\n",
    "\n",
    "        print('\\rEpoch:{} Batch:{}/{} Loss:{:.4f} Time:{}m{:.2f}s'.format(epoch + 1, batch_id, \n",
    "                                                                          num_batch, loss.item(),\n",
    "                                                                          elapsed_min, elapsed_sec), end='')\n",
    "    print()\n",
    "    mlp.eval()\n",
    "    valid_loss = 0\n",
    "    best_loss = np.inf\n",
    "    num_batch = valid_dataloader.data_size // valid_dataloader.batch_size + 1\n",
    "    \n",
    "    for batch_id, (batch_x, batch_y) in enumerate(valid_dataloader):\n",
    "    \n",
    "        y_pred = mlp(batch_x)\n",
    "        loss = mae_loss(y_pred, batch_y)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    valid_loss /= num_batch\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    #save model when validation loss is minimum\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(mlp.state_dict(), 'mlp.model')  \n",
    "    \n",
    "    print('Valid Loss:{:.4f}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum validation loss is 2453.5182\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot validation loss curve, this may help to notice overfitting\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.ylim(0,max(valid_losses)+0.02)\n",
    "plt.plot(valid_losses)\n",
    "print('minimum validation loss is {:.4f}'.format(min(valid_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-961e1ad2df57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'DataFrame'"
     ]
    }
   ],
   "source": [
    "y_test = mlp(torch.tensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
